# Telegram-бот с локальной LLM (LM Studio) и контекстом

Я использую ```uv``` для управления зависимостями и запуском.

## Что нужно перед стартом

1. **Python** (3.10+ желательно)
2. **uv**  
   Если ещё нет:

   ```bash
   pip install uv
   ```

3. **LM Studio**
    - Скачайте и поставьте LM Studio.
    - Загрузите любую чат-модель (например, Llama, Mistral и т.п.).
    - Запустите **Local Server** (OpenAI-compatible API).
    - Оставьте адрес по умолчанию: `http://localhost:1234`.

4. **Телеграм-бот**
    - Создате нового бота и получите токен для использование API Telegram.

## Структура проекта

- `main.py` — точка входа, просто запускает бота.
- `bot_app.py` — вся логика бота, хендлеры, запросы к LM Studio.
- `db.py` — работа с SQLite, хранение контекста.
- `bot.db` — база, создаётся автоматически при первом запуске. (у меня в гитигноре)
- `.env-example` – пример env-файла, в котором нужно указать токен.

###   

```shell
mv .env-example .env
vim .env
```

Далее вставить токен и сохранить файл (в случае использования vim следующей комбинацией)

```shell
:q
```

---

## Установка зависимостей (через uv)

Из корня проекта:

```bash
uv sync
```

# Запуск бота

1. Запуск бота:

```bash
uv run python main.py
```

- `/start` — краткая помощь, что это за бот и что он умеет.
- `/model` — показывает название модели, которая используется.
- `/clear` — очищает контекст пользователя.
- Любой обычный текст — уходит в LLM с учётом накопленного контекста.
